## Fine Tune DeepSeek

#### 1.参考材料：

1. Fine-Tuning DeepSeek R1 (Reasoning Model) ：https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model
2. Unsloth微调框架：https://github.com/unslothai/unsloth
3. Rosenberg/CMeEE-V2数据集：https://huggingface.co/datasets/Rosenberg/CMeEE-V2
4. FreedomIntelligence/medical-o1-reasoning-SFT数据集：https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT
5.  DeepSeek- R1-Distill-Llama-8B模型（HuggingFace）：[DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B).
6. Uncloth-SFT 文档：https://huggingface.co/docs/trl/main/en/sft_trainer
7. 

#### 2.任务目标：

我们将对 DeepSeek-R1-Distill-Llama-8B 模型进行微调，该过程将在 Hugging Face 的 Medical Chain-of-Thought 数据集上完成。这个经过蒸馏的 DeepSeek-R1 模型是通过在使用 DeepSeek-R1 生成的数据上对 Llama 3.1 8B 模型进行微调而创建的，它展示了与原始模型相似的推理能力。

#### 3.模型介绍

##### 3.1 知识蒸馏

知识蒸馏就是把一个大的教师模型的知识萃取出来，把他浓缩到一个小的学生模型，可以理解为一个大的教师神经网络把他的知识教给小的学生网络，这里有一个知识的迁移过程，从教师网络迁移到了学生网络身上，教师网络一般是比较臃肿，所以教师网络把知识教给学生网络，学生网络是一个比较小的网络，这样就可以用学生网络去做一些轻量化网络做的事情。

![img](./assets/2e72310670a7278f00d036ab26b82490.png)

知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:

- **原始模型训练:** 训练"Teacher模型", 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对"Teacher模型"不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。
- **精简模型训练:** 训练"Student模型", 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。
- Teacher学习能力强，可以将它学到的知识迁移给学习能力相对弱的Student模型，以此来增强Student模型的泛化能力。复杂笨重但是效果好的Teacher模型不上线，就单纯是个导师角色，真正部署上线进行预测任务的是灵活轻巧的Student小模型。

#### 4. Unsltoh介绍

Unsloth 是一款开源的 LLM 微调加速工具，它以其独特的技术实现，在人工智能领域掀起了一场性能与效率的革命。

1. **加速微调：大幅提升微调速度**

相比传统方法，Unsloth 可以将微调速度提高 2 倍左右，甚至2倍以上的加速。这一惊人的速度提升，使得用户能够在更短的时间内完成模型的微调，大大提高了工作效率。

例如，在处理大规模数据集时，传统的微调方法可能需要数天甚至数周的时间，而 Unsloth 则可以将这个时间缩短到几个小时甚至更短。这对于那些需要快速迭代和优化模型的用户来说，无疑是一个巨大的福音。

2. **降低内存使用：突破资源限制**

Unsloth 能够减少高达 80%的内存占用，这一特性使得在有限硬件资源下训练更大的模型成为可能。在人工智能领域，模型的规模往往与性能成正比，但同时也会带来巨大的内存需求。Unsloth 的出现，为那些资源受限的用户提供了一种解决方案，让他们能够在不增加硬件成本的情况下，训练出更强大的模型。

例如，在一些小型企业或个人开发者的环境中，硬件资源可能相对有限。Unsloth 的低内存占用特性，使得他们也能够参与到大规模模型的训练和微调中来，为人工智能的发展贡献自己的力量。

3. **广泛的模型支持：兼容多种主流 LLM**

Unsloth 支持多种主流 LLM，包括 Llama 3.1、Mistral、Phi - 3.5 和 Gemma 等。这意味着用户可以在不同的语言模型之间进行选择，根据自己的具体需求和任务特点，找到最适合的模型进行微调。

同时，广泛的模型支持也为用户提供了更多的实验和探索空间。他们可以尝试不同的模型和微调方法，从中找到最佳的组合，以实现更好的性能和效果。

#### 5.关于UnSloth微调框架的优势

1）加载模型和分词器

代码中基于 `unsloth` 的 `FastLanguageModel.from_pretrained()` 加载了模型和分词器

```python
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)
```

基于Hugging Face的 `transformers` 的模型和分词器加载方式，以此来对比一下

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = './model/llama-3-8b'   # 模型的本地路径
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map='auto')
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
```

2）LORA Adapter

1）代码中基于 `unsloth` 的 `FastLanguageModel.get_peft_model()` 的方式增加了 LoRA adapter，后续该模型作为参数传入 `SFTTrainer` 中。

```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

```

2） 本文也给出传统的基于 `peft` 的`get_peft_model` 的增加LoRA adapter 的方式，以此来对比一下

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj",],
)

model = get_peft_model(model, lora_config)
# model.print_trainable_parameters()
```

3）性能优势

![image-20250218155102682](./assets/image-20250218155102682.png)

#### 6. 微调类型



#### 7.微调情况



![image-20250220084602566](./assets/image-20250220084602566-0012365.png)