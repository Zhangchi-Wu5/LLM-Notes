## Fine Tune DeepSeek

#### 1.参考材料：

1. Fine-Tuning DeepSeek R1 (Reasoning Model) ：https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model
2. Unsloth微调框架：https://github.com/unslothai/unsloth
3. Rosenberg/CMeEE-V2数据集：https://huggingface.co/datasets/Rosenberg/CMeEE-V2
4. FreedomIntelligence/medical-o1-reasoning-SFT数据集：https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT
5.  DeepSeek- R1-Distill-Llama-8B模型（HuggingFace）：[DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B).

#### 2.任务目标：

我们将对 DeepSeek-R1-Distill-Llama-8B 模型进行微调，该过程将在 Hugging Face 的 Medical Chain-of-Thought 数据集上完成。这个经过蒸馏的 DeepSeek-R1 模型是通过在使用 DeepSeek-R1 生成的数据上对 Llama 3.1 8B 模型进行微调而创建的，它展示了与原始模型相似的推理能力。

#### 3. Unsltoh介绍

Unsloth 是一款开源的 LLM 微调加速工具，它以其独特的技术实现，在人工智能领域掀起了一场性能与效率的革命。

1. **加速微调：大幅提升微调速度**

相比传统方法，Unsloth 可以将微调速度提高 2 倍左右，甚至2倍以上的加速。这一惊人的速度提升，使得用户能够在更短的时间内完成模型的微调，大大提高了工作效率。

例如，在处理大规模数据集时，传统的微调方法可能需要数天甚至数周的时间，而 Unsloth 则可以将这个时间缩短到几个小时甚至更短。这对于那些需要快速迭代和优化模型的用户来说，无疑是一个巨大的福音。

2. **降低内存使用：突破资源限制**

Unsloth 能够减少高达 80%的内存占用，这一特性使得在有限硬件资源下训练更大的模型成为可能。在人工智能领域，模型的规模往往与性能成正比，但同时也会带来巨大的内存需求。Unsloth 的出现，为那些资源受限的用户提供了一种解决方案，让他们能够在不增加硬件成本的情况下，训练出更强大的模型。

例如，在一些小型企业或个人开发者的环境中，硬件资源可能相对有限。Unsloth 的低内存占用特性，使得他们也能够参与到大规模模型的训练和微调中来，为人工智能的发展贡献自己的力量。

3. **广泛的模型支持：兼容多种主流 LLM**

Unsloth 支持多种主流 LLM，包括 Llama 3.1、Mistral、Phi - 3.5 和 Gemma 等。这意味着用户可以在不同的语言模型之间进行选择，根据自己的具体需求和任务特点，找到最适合的模型进行微调。

同时，广泛的模型支持也为用户提供了更多的实验和探索空间。他们可以尝试不同的模型和微调方法，从中找到最佳的组合，以实现更好的性能和效果。

#### 3.关于UnSloth微调框架的优势

1）加载模型和分词器

代码中基于 `unsloth` 的 `FastLanguageModel.from_pretrained()` 加载了模型和分词器

```python
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)
```

基于Hugging Face的 `transformers` 的模型和分词器加载方式，以此来对比一下

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = './model/llama-3-8b'   # 模型的本地路径
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map='auto')
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
```

2）LORA Adapter

1）代码中基于 `unsloth` 的 `FastLanguageModel.get_peft_model()` 的方式增加了 LoRA adapter，后续该模型作为参数传入 `SFTTrainer` 中。

```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

```

2） 本文也给出传统的基于 `peft` 的`get_peft_model` 的增加LoRA adapter 的方式，以此来对比一下

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj",],
)

model = get_peft_model(model, lora_config)
# model.print_trainable_parameters()
```

3）性能优势

![image-20250218155102682](/Users/wuzhangchi/Documents/AI/LLM-Notes/10. LLM实战/assets/image-20250218155102682.png)